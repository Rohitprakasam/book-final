# ============================================================
# BookUdecate V1.0 â€” Environment Configuration Template
# ============================================================
# Copy this file to .env and fill in your API keys.
# ============================================================

# --- LLM Provider Keys ---
# Set at least one to use Gemini (Cloud) or Ollama (Local)
GOOGLE_API_KEY=your_gemini_api_key_here
# GEMINI_API_KEY=your_gemini_api_key_here (Alias)

# --- AI Provider Configuration ---
# Options: gemini, ollama
LLM_PROVIDER=gemini
# If using Ollama, set the base URL (e.g., http://192.168.1.5:11434)
OLLAMA_API_BASE=http://localhost:11434

# --- Model Selection (LiteLLM) ---
# Primary model for text expansion
DEFAULT_MODEL=gemini/gemini-2.0-flash
# Optimization: Route math-heavy chunks to a more capable model
MATH_MODEL=gemini/gemini-1.5-pro
# Optimization: Fast model for simple prose
FLASH_MODEL=gemini/gemini-2.0-flash

# --- Image Generation ---
# Model used for generating technical diagrams
IMAGE_MODEL=gemini-1.5-flash
# Skip AI images and use placeholders? (true/false)
SKIP_IMAGES=false
# Maximum number of new diagrams to generate per job
MAX_NEW_DIAGRAMS=20

# --- Book Personality & Subject ---
BOOK_SUBJECT=Engineering
BOOK_PERSONA=Elite Professor specializing in the subject matter
ACADEMIC_LEVEL=Undergraduate Course

# --- Server Configuration ---
PORT=8000
HOST=0.0.0.0

# --- Performance & Concurrency ---
# Number of parallel chunks to expand (Adjust based on rate limits)
EXPANSION_CONCURRENCY=15
# Number of parallel JSON structures to build
STRUCTURER_CONCURRENCY=10
